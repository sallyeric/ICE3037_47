{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "NewsCrawl.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPWTBeh9fWck"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import os "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULGy23mZezqk",
        "outputId": "96486d87-4925-49af-e5eb-61e89137cc4c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YcwrouLfRMK"
      },
      "source": [
        "# 경로 지정\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8u87VuVePwT",
        "outputId": "14f2dfcb-d683-4d57-ed2e-77fe7aa4728c"
      },
      "source": [
        "# def crawler(company_code, maxpage):\n",
        "def crawler(company, companyCode, startDate, endDate, maxpage):\n",
        "    page = 1 \n",
        "    \n",
        "    while page <= int(maxpage):\n",
        "#         url = 'https://finance.naver.com/item/news_news.nhn?code=' + str(company_code) +'&page=' + str(page) \n",
        "        url = 'https://finance.naver.com/news/news_search.nhn?q='+str(companyCode)+'&x=0&y=0&sm=all.basic&pd=4&stDateStart=' + str(startDate) + '&stDateEnd=' + str(endDate) + '&page=' + str(page)\n",
        "        source_code = requests.get(url).text\n",
        "        html = BeautifulSoup(source_code, \"lxml\")\n",
        "\n",
        "        # #회사명\n",
        "        # company_result=[]\n",
        "        # for title in titles:\n",
        "        #   company_result.append(company)\n",
        "\n",
        "        # 회사명 & 뉴스 제목\n",
        "#         titles =  html.find_all('.title')\n",
        "        \n",
        "        titles = [p for p in html.find_all(class_='articleSubject')]\n",
        "        title_result=[]\n",
        "        company_result=[]\n",
        "        for title in titles:\n",
        "            title = title.get_text() \n",
        "            title = re.sub('\\n','',title)\n",
        "            title = re.sub('\\t','',title)\n",
        "            title_result.append(title)\n",
        "            company_result.append(company)\n",
        "            \n",
        "        \n",
        "        # 뉴스 링크\n",
        "        links = [a for a in html.find_all(class_='articleSubject')]\n",
        "        link_result =[]\n",
        "        for link in links: \n",
        "            add = 'https://finance.naver.com' + link.find('a')['href']\n",
        "            link_result.append(add)\n",
        "\n",
        "        # 뉴스 날짜 \n",
        "        dates = html.select('.wdate')\n",
        "        date_result = [date.get_text() for date in dates]\n",
        "        for i in range(len(date_result)):\n",
        "            date_result[i] = re.sub('\\t','',date_result[i])\n",
        "            date_result[i] = re.sub('\\n','',date_result[i])\n",
        "        \n",
        "        # 뉴스 매체\n",
        "        sources = html.select('.press')\n",
        "        source_result = [source.get_text() for source in sources]\n",
        "        \n",
        "        # 뉴스 본문\n",
        "        links = [a for a in html.find_all(class_='articleSubject')]\n",
        "        content_result =[]\n",
        "        for link2 in links:\n",
        "            add2 = 'https://finance.naver.com' + link2.find('a')['href']\n",
        "            url2 = add2\n",
        "            \n",
        "            source_code2 = requests.get(url2).text\n",
        "            html2 = BeautifulSoup(source_code2, \"lxml\")\n",
        "            contents = html2.select('.boardView')\n",
        "            content_whole = [content.get_text() for content in contents] \n",
        "            for i in range(len(content_whole)):\n",
        "                content_whole[i] = re.sub('\\t','',content_whole[i])\n",
        "                content_whole[i] = re.sub('\\n','',content_whole[i])\n",
        "        \n",
        "            content_result.append(content_whole)\n",
        "\n",
        "        # 회사 영문명\n",
        "        company_eng_name = convert_to_eng(company)\n",
        " \n",
        "        # 변수들 합쳐서 해당 디렉토리에 csv파일로 저장하기 \n",
        " \n",
        "        result= {\"회사명\" : company_result, \"날짜\" : date_result, \"언론사\" : source_result, \"기사제목\" : title_result, \"링크\" : link_result ,\"본문\":content_result} \n",
        "        df_result = pd.DataFrame(result)\n",
        "        \n",
        "        print(\"다운 받고 있습니다------\")\n",
        "        df_result.to_csv(str(startDate) + 'page' + str(page) + str(company_eng_name) + '.csv', mode='a', encoding='utf-8-sig') \n",
        "            \n",
        "        page += 1\n",
        "    print('끝')\n",
        "\n",
        "# 회사 영문명으로 변경\n",
        "def convert_to_eng(company):\n",
        "    data = pd.read_csv('company_eng.txt', dtype=str, sep='\\t') \n",
        "    company_kor = data['회사명']\n",
        "    keys = [i for i in company_kor]    #데이터프레임에서 리스트로 바꾸기 \n",
        " \n",
        "    company_eng = data['영문명']\n",
        "    values = [j for j in company_eng]\n",
        " \n",
        "    dict_result = dict(zip(keys, values))  # 딕셔너리 형태로 회사이름과 종목코드 묶기 \n",
        "    \n",
        "    pattern = '[a-zA-Z가-힣]+' \n",
        "    \n",
        "    if bool(re.match(pattern, company)) == True:         # Input에 이름으로 넣었을 때  \n",
        "        company_eng = dict_result.get(str(company))\n",
        "        return company_eng\n",
        "                             \n",
        "# 종목 리스트 파일 열기  \n",
        "# 회사명을 종목코드로 변환 \n",
        "\n",
        "def convert_to_code(company, startDate, endDate, maxpage):\n",
        "    \n",
        "    data = pd.read_csv('company_code.txt', dtype=str, sep='\\t')   # 종목코드 추출 \n",
        "    company_name = data['회사명']\n",
        "    keys = [i for i in company_name]    #데이터프레임에서 리스트로 바꾸기 \n",
        " \n",
        "    company_code = data['종목코드']\n",
        "    values = [j for j in company_code]\n",
        " \n",
        "    dict_result = dict(zip(keys, values))  # 딕셔너리 형태로 회사이름과 종목코드 묶기 \n",
        "    \n",
        "    pattern = '[a-zA-Z가-힣]+' \n",
        "    \n",
        "    if bool(re.match(pattern, company)) == True:         # Input에 이름으로 넣었을 때  \n",
        "        company_code = dict_result.get(str(company))\n",
        "        crawler(company, company_code,startDate, endDate, maxpage)\n",
        " \n",
        "    \n",
        "    # else:                                                # Input에 종목코드로 넣었을 때       \n",
        "    #     company_code = str(company)      \n",
        "    #     crawler(company_code, startDate, endDate, maxpage)\n",
        "\n",
        "\n",
        "def main():\n",
        "    info_main = input(\"=\"*50+\"\\n\"+\"실시간 뉴스기사 다운받기.\"+\"\\n\"+\" 시작하시려면 Enter를 눌러주세요.\"+\"\\n\"+\"=\"*50)\n",
        "    \n",
        "    company = input(\"회사명: \")\n",
        "    \n",
        "    startDate = input(\"시작날짜: \")\n",
        "\n",
        "    endDate = input(\"마지막날짜: \")\n",
        "    \n",
        "    maxpage = input(\"최대 뉴스 페이지 수 입력: \")\n",
        "\n",
        "    convert_to_code(company, startDate, endDate, maxpage)\n",
        "\n",
        "    #crawler(startDate, endDate, maxpage)\n",
        " \n",
        "    # convert_to_code(company, maxpage)\n",
        "\n",
        "main()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "실시간 뉴스기사 다운받기.\n",
            " 시작하시려면 Enter를 눌러주세요.\n",
            "==================================================\n",
            "회사명: 셀트리온\n",
            "시작날짜: 2017-01-01\n",
            "마지막날짜: 2017-01-01\n",
            "최대 뉴스 페이지 수 입력: 3\n",
            "다운 받고 있습니다------\n",
            "다운 받고 있습니다------\n",
            "다운 받고 있습니다------\n",
            "끝\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEgZSBGrePwb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}